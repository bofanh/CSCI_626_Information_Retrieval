{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2jZ5jAw1QnBL"},"outputs":[],"source":["# History\n","# Name      Date          Description                   Issue\n","# Bofan     Nov 20        upload and load dataset.      file too large to load and causing crash\n","# Emanuel   Dec 1         removing reviews stopwords    "]},{"cell_type":"markdown","metadata":{"id":"8jo1sXN-HI2K"},"source":["# 1. Preprocessing  dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15OvC2i7F_CK"},"outputs":[],"source":["# Preprocess data: removing irrelevant features\n","# Includes: tokenization, case-folding, stopword removal, stem words, weight words\n","# Includes: tokenization, case-folding, stop word removal\n","# Return a ranked list of reviews \n","\n","# dataset download link: https://jmcauley.ucsd.edu/data/amazon_v2/index.html\n","# example name is sport and oudoor"]},{"cell_type":"markdown","metadata":{"id":"-2U5eUTOS5Ss"},"source":["## Bofan      load dataset"]},{"cell_type":"code","execution_count":138,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":987,"status":"ok","timestamp":1669957245033,"user":{"displayName":"Bofan He","userId":"02058615448883407176"},"user_tz":300},"id":"6yF7LrJ2GMzz","outputId":"b07c4015-90c0-4758-b150-2e60ee3ef42b"},"outputs":[],"source":["from os import path\n","import os\n","import pandas as pd\n","import numpy as np\n","import pandas as pd\n","import missingno as msno\n","\n","import matplotlib.pyplot as plt\n","import glob\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KmxVGjKCH5Jj"},"source":["#### read df"]},{"cell_type":"code","execution_count":128,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8457,"status":"ok","timestamp":1669960453168,"user":{"displayName":"Bofan He","userId":"02058615448883407176"},"user_tz":300},"id":"WnZwO26_Efek","outputId":"3d99184c-2f75-41cd-ca98-9a0c7be9995c"},"outputs":[],"source":["################## \n","'''\n","Create sample data and chunks\n","'''\n","##################\n","\n","\n","# generate sample data\n","\n","#filename = location+path\n","#samplepath = location+sample\n","\n","# for i in chunks:\n","#   df = i\n","#   break\n","# # save to sample.csv in data folder\n","# df.to_csv(samplepath)\n","\n","\n","\n","# generate a smaller chunk dataset for futrue use\n","import pandas as pd\n","\n","\n","def sampledata(filename,chunkpath, chunksize):\n","  chunks = pd.read_json(filename, lines=True, chunksize = chunksize)\n","  i=0\n","  for c in chunks:\n","      #print(c)\n","      i += 1\n","      print(i,\"th of\",str(chunksize))\n","      c.to_csv(chunkpath+str(i)+'.csv')\n","\n","################## \n","'''\n","read chunks and write back\n","'''\n","##################\n","\n","def printdfsize(df):  # get data frame size by compute row * col\n","    row, col = df.shape\n","    print('df size', row * col)\n","\n","\n","def cleanchunk(chunkfilepath):\n","    # chunkfilepath = 'data/chunkdata/chunk1.csv'\n","\n","    # read and load sample.csv data\n","    df = pd.read_csv(chunkfilepath, low_memory=True)\n","    print('#'*20,  'dataset load in shape')\n","    print(df.shape)\n","\n","    printdfsize(df)\n","\n","    # print(df.head())\n","    print( '#'*20, 'column names')\n","    print(list(df.columns))\n","    # msno.matrix(df)\n","\n","    df = df.drop(columns=['Unnamed: 0','vote', 'image','style','reviewTime','reviewerName','Unnamed: 0', 'reviewerID'])\n","    df = df.dropna()\n","    print('#'* 20, 'shape after first drop')\n","    print(df.shape)\n","    printdfsize(df)\n","\n","    print('#'* 20, 'shape after drop false verified')\n","    df = df[df.verified == True]\n","    df = df.drop(columns=['verified'])\n","    print(df.shape)\n","    printdfsize(df)\n","    # print(list(df.columns))\n","    # df.head()\n","    # df['overall'].value_counts()\n","\n","    # remove rows that has overall rate lower than 4 \n","    ratestd = 4\n","    df = df[df.overall > ratestd-1]\n","    print('#'* 20, 'df shape after filtering rate lower than',str(ratestd),'is')\n","    print( df.shape)\n","    printdfsize(df)\n","    df.head()\n","\n","    # df\n","\n","    df.to_csv(chunkfilepath) \n","\n","\n","def writenewchunk(list_of_files):  # after clean data over write chunk.csv \n","    for i in (list_of_files):\n","        print('-'*100)\n","        print(\"clean chunk \",str(i))\n","        print('-'*100)\n","        chunkfilepath = 'data/chunkdata/'+i\n","        cleanchunk(chunkfilepath)\n","\n","\n","################## \n","'''\n","mergy chunks into one df\n","'''\n","##################\n","\n","\n","def chunkstoone():\n","    # setting the path for joining multiple files\n","    files = os.path.join(\"data/chunkdata/\", \"chunk*.csv\")\n","\n","    # list of merged files returned\n","    files = glob.glob(files)\n","\n","    print(\"Resultant CSV after joining all CSV files at a particular location...\");\n","\n","    # joining files with concat and read_csv\n","    df = pd.concat(map(pd.read_csv, files), ignore_index=False)\n","    df = df.drop(columns=['Unnamed: 0'])\n","    print( df.shape)\n","    printdfsize(df)\n","    return df\n","\n","\n","################## \n","'''\n","asin\n","'''\n","##################\n","\n","def checkasinreviewcount(df):   # df['asin'].value_counts()\n","    df['asin'].value_counts().plot()\n","    plt.title('product\\'s review count')\n","    plt.ylabel('number of review')\n","    plt.xlabel('product\\'s ASIN')\n","\n","\n","def createasinlist(numbersofreviews, df): # create condition list of product that has threshold number of reviews and plot\n","    asincount = list()\n","    for i in range(numbersofreviews):\n","        asin = df['asin'].value_counts()\n","        asin = asin.to_frame()\n","        asinlist= asin.index[asin['asin'] >i].tolist()\n","        a = len(asinlist)\n","        asincount.append(a)\n","    # plt.plot(asincount)\n","    # plt.title('number of product for certain number of review')\n","    # plt.ylabel('number of product')\n","    # plt.xlabel('number of review per product (review/product)')\n","    return asinlist\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main():\n","\n","    # B\n","    # dataset web address https://jmcauley.ucsd.edu/data/amazon_v2/categoryFiles/Sports_and_Outdoors.json.gz\n","\n","    location = 'data/'\n","    chunklocation = 'chunkdata/chunk'\n","    path = 'Sports_and_Outdoors.json'\n","    #pathgz = 'Sports_and_Outdoors.json.gz' \n","    # sample = 'sample'\n","    # file location and name\n","\n","    #filenamegz = location+pathgz\n","    filename = location + path\n","    chunkpath = location + chunklocation\n","\n","    print(filename)\n","    print(chunkpath)\n","\n","    # os.mkdir('data/chunkdata')\n","\n","    print(chunkpath+str(0)+'.csv')\n","\n","    #####################################\n","    # create chunks from original dataset\n","    sampledata(filename,chunkpath, chunksize = 10**5)\n","\n","\n","    # drop columns and write backto chunks\n","    list_of_files = os.listdir('data/chunkdata/')\n","    # print(list_of_files)\n","    writenewchunk(list_of_files)\n","\n","\n","    # read chunks in one data frame\n","    df = chunkstoone()\n","\n","    # select rows of data that asin has review at least of review number.\n","    # then write is as prep.csv for ML session\n","    reviewnumber = 200\n","    asinlist = createasinlist(reviewnumber,df)\n","\n","    df = df[df['asin'].isin(asinlist)]\n","    print('#'* 20, 'df shape after filtering product review number lower than',str(reviewnumber),'is')\n","    print( df.shape)\n","    printdfsize(df)\n","    df.to_csv('data/prep.csv')"]},{"cell_type":"markdown","metadata":{"id":"exDbLgV-NeeX"},"source":["### E"]}],"metadata":{"colab":{"collapsed_sections":["6w8wsSE5Hg1N"],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.15 ('nlp')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"vscode":{"interpreter":{"hash":"5dd83c0049ca90416432c7bf31543fa4a43c6d1cbcb766d79372bfcb46406b7f"}}},"nbformat":4,"nbformat_minor":0}
